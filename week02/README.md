ğŸš€ Week 2: Diving Deeper into Machine Learning

This week was all about applying what Iâ€™ve learned so far â€” from structured supervised models to exploring the creative side of unsupervised learning.
I focused on two key projects and started shaping my understanding of Deep Learning foundations through hands-on and writing exercises.


---

â¤ï¸ Heart Disease Prediction â€” Supervised Learning in Action

Objective: Predict whether a person is likely to have heart disease based on medical parameters.

Key Steps:

1. Performed detailed Exploratory Data Analysis (EDA) to understand data patterns and relationships.


2. Built models using Logistic Regression, Decision Tree, and Random Forest to compare performance.


3. Evaluated results using Accuracy, Recall, and F1-Score, along with Confusion Matrix interpretation.


4. Discovered how Random Forest outperformed others with higher accuracy and generalization.



Takeaway:
Structured data + right preprocessing + interpretability = a powerful first step in health-based predictions.

ğŸ“˜ Accompanying Blog: [Can Machines Help Predict Heart Disease?](https://medium.com/@aiwithaditya/can-machines-help-predict-heart-disease-98f3877159e2)


---

ğŸŒ¸ Iris Clustering â€” Understanding Unsupervised Learning

Objective: Learn how clustering and dimensionality reduction uncover hidden structures in data.

Key Steps:

1. Explored the Iris dataset using pair plots and scatter plots for visual intuition.


2. Implemented K-Means Clustering, followed by PCA (Principal Component Analysis) to reduce features to 2D.


3. Used Silhouette Score, Confusion Matrix, and Hungarian Algorithm for cluster evaluation.


4. Compared results â€” original features gave slightly better accuracy, PCA offered clearer visual separation.



Takeaway:
Unsupervised learning is about seeing the unseen â€” not prediction, but discovery.

ğŸ“˜ Accompanying Blog: [Clustering Flowers with KMeans and PCA](https://medium.com/@aiwithaditya/clustering-flowers-with-kmeans-and-pca-bdf33753dae9)


---

ğŸ§  Deep Learning Foundations â€” Laying the Groundwork

Focus:

Studied the intuition behind Artificial Neural Networks (ANNs).

Understood how neurons, weights, biases, and activations form the backbone of deep models.

Outlined my roadmap for the upcoming week: implementing a basic ANN from scratch using NumPy before moving to frameworks.


Takeaway:
Before diving into big models, understand the smallest unit â€” the neuron.

ğŸ“˜ Story Telling Blog : [â€œFrom Thought to Codeâ€Š-â€ŠVikram and Vetal's Journey into DeepÂ Learningâ€](https://medium.com/@aiwithaditya/from-thought-to-code-vikram-and-vetals-journey-into-deep-learning-02cff079dee5)


---

âœï¸ Reflection & Learning Highlights

ğŸ”¹ This week solidified my understanding of how models learn from patterns â€” both labeled and unlabeled data.
ğŸ”¹ Visualization continues to be my strongest exploration tool.
ğŸ”¹ Iâ€™ve started focusing on explaining models simply through blogs â€” bridging learning and teaching.


---

ğŸ Whatâ€™s Next

â¡ï¸ Week 3: Implementing my first Neural Network, exploring regularization, and experimenting with feature selection techniques.
â¡ï¸ Continue writing and refining blogs that simplify ML concepts for others following a similar path.

## [CLICK HERE](https://github.com/adityanarayan007/ml-portfolio/tree/main) TO RETURN TO MAIN PAGE
---

ğŸ”– Tags

#MachineLearning #DataScience #UnsupervisedLearning #SupervisedLearning #DeepLearning #KMeans #PCA #LogisticRegression #RandomForest #LearningJourney
